Only need to run once
1. apply_brain_msk.sh
2. get_confounds.sh w/get_confounds.py

Need to run for every new model
3. run_level1.sh w/ *_maker.sh
3a. get_BIC.sh w/get_BIC.py if doing a model comparison approach
3b. BIC_setup_randomise.sh
3c. BIC_randomise.sh 
3d. 2nd_level_randomise.sh
3e. BIC_model_comp.py
3f. entropy_subval_align.py

4. setup_level2.sh
5. run_level2.sh w/ *_maker.sh
6. run_level3.sh w/ *_maker.sh
7. masking.sh

#check number of files per dir
fldrs=(*)
for fldr in ${fldrs[@]}
do

#echo $fldr

ls ${fldr} | wc -l

done

#erase filtered fmri images
fldrs=(*)
for fldr in ${fldrs[@]}
do

cd $fldr

rm filtered_func_data.nii.gz

cd ..

done


#Use this to tar up models
fldrs=(*.feat)
for fldr in ${fldrs[@]}
do

tar -czvf ${fldr}.tar.gz $fldr

done

#Use this to untar up models
tarballs=(*)
for tarball in ${tarballs[@]}
do

tar -zxvf ${tarball}

done

#Use this to check errors on jobs
cd /scratch/scratch/ucjuogu/NARPS2/derivatives/fmriprep
sub_fldrs=(sub*/)
cd /scratch/scratch/ucjtbob/narps_level2_logs

for i in {1..108}
do
otheri=$( expr $i - 1 )
echo ${sub_fldrs[$otheri]} job_num $i
cat narps_level2.e471485.${i}
done

#use this also to check errors on jobs
sub_fldrs=(sub*/)
for sub_fldr in ${sub_fldrs[@]}
do

echo $sub_fldr

if [ ! -d "${sub_fldr}/stats" ]; then
  echo 'stats directory does not exist'
fi

done


#Use this to delete filtered_func

sub_fldrs=(sub*/)

for sub_fldr in ${sub_fldrs[@]}
do

echo $sub_fldr
rm ${sub_fldr}/filtered_func_data.nii.gz

done

#check logs one by one


files_of_interest=(narps_level1_run4.e*)

for file_of_interest in ${files_of_interest[@]}
do

echo $file_of_interest
cat $file_of_interest

done








